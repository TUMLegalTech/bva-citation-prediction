{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "pGDcL5nkE-Nf"
      },
      "source": [
        "This section imports libraries and contains some utility functions. Ideally, these utility functions should be in a common library on the git repo. However, it is not trivial to sync the Colab workflow with the github repository, so currently each Colab notebook is self-contained (i.e. does not import scripts from the github repo)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "n-c-D8vi5IWd"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# Libraries and Utility Functions\n",
        "!pip install fastavro\n",
        "from dask.distributed import Client, progress\n",
        "import numpy as np\n",
        "import dask.bag as db\n",
        "import time\n",
        "import os\n",
        "import re\n",
        "import random\n",
        "import glob\n",
        "from dask.distributed import Client, LocalCluster\n",
        "import fastavro\n",
        "import pandas as pd\n",
        "\n",
        "# For some reason, Dask does not work well with the inline %timeit function, so use a simple timer\n",
        "class Timer():\n",
        "    def start(self):\n",
        "        self.start_time = time.time()\n",
        "\n",
        "    def end(self):\n",
        "        self.end_time = time.time()\n",
        "        print(f\"Time elapsed: {self.end_time - self.start_time:.2f} seconds.\")\n",
        "\n",
        "# Read text into dask bag\n",
        "def load_case_documents(files, npartitions=100):\n",
        "    def load_from_filename(file):\n",
        "        with open(file, errors=\"ignore\", encoding=\"utf-8\") as f:\n",
        "            filename = file.split(\"/\")[-1].split(\".\")[0]                # Get filename between parent_directory/ and .txt\n",
        "            return {\"bva_id\": int(filename), \"text\" : f.read()}\n",
        "    b = db.from_sequence(files, npartitions=npartitions).map(load_from_filename)\n",
        "    return b\n",
        "\n",
        "# Init timer\n",
        "timer = Timer()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "CM4pu890DeNS"
      },
      "source": [
        "Here we identify the files that are single-issue documents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "colab_type": "code",
        "id": "H1EcUBo9DcoP",
        "outputId": "89a82ae4-db09-4906-b5ff-0e3ff09bcb76"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['all-bva-decisions', '9221305a']\n"
          ]
        }
      ],
      "source": [
        "# Some preprocessing on the documents\n",
        "documents = [x.split(\".\")[0] for x in os.listdir(\"all-bva-decisions\")]\n",
        "print(list(filter(lambda x: not str.isdigit(x), documents)))  \n",
        "!mv all-bva-decisions/9221305a.txt all-bva-decisions/9221305.txt  # There is one document with \"a\" appended\n",
        "!rm all-bva-decisions/all-bva-decisions.tar.gz                    # There is another tar.gz inside\n",
        "documents = np.array(os.listdir(\"all-bva-decisions\"))\n",
        "documents_int = np.array([x.split(\".\")[0] for x in documents], dtype=np.int64)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "colab_type": "code",
        "id": "RKaSJml2NMUO",
        "outputId": "1c41c603-a29b-45bb-c227-2d9c81aadc90"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "All overlapping columns between vacols and bva are identical.\n"
          ]
        }
      ],
      "source": [
        "# Load BVACases and Vacols\n",
        "dict_types = {\"appeal_id\": str, \n",
        "              \"tiread2\": np.int64, \n",
        "              \"issue_id\": str, \n",
        "              \"imgadtm\": str,\n",
        "              \"issdc\": str,\n",
        "              \"issseq\": str,\n",
        "              \"issprog\": str,\n",
        "              \"isscode\": str,\n",
        "              \"isslev2\": str,\n",
        "              \"isslev3\": str,\n",
        "              \"cvdocket\": str,\n",
        "              \"cvdisp\": str,\n",
        "              \"appealed_CAVC\": np.int32,\n",
        "              \"issue_count\": np.int32}\n",
        "\n",
        "bva = pd.read_csv(\"BVACases.csv\", dtype=dict_types)\n",
        "bva = bva.sort_values(\"appeal_id\").reset_index(drop=True)\n",
        "bva.fillna(\"na\", inplace=True)\n",
        "\n",
        "vacols = pd.read_csv(\"updated_vacols.csv\", dtype=dict_types)\n",
        "vacols.columns.values[0] = \"citation_num\"\n",
        "vacols = vacols.sort_values(\"appeal_id\").reset_index(drop=True)\n",
        "vacols.fillna(\"na\", inplace=True)\n",
        "\n",
        "# Check equality between bva and vacols\n",
        "# Yes, all overlapping columns are identical \n",
        "overlapping_cols = list(set(vacols.columns) & set(bva.columns))\n",
        "any_diff = False\n",
        "for col in overlapping_cols:\n",
        "    diff = np.sum(bva[col] != vacols[col])\n",
        "    if diff > 0:\n",
        "        print(f\"{col}: {diff} rows differ.\")\n",
        "        any_diff = True\n",
        "if not any_diff:\n",
        "    print(\"All overlapping columns between vacols and bva are identical.\")\n",
        "\n",
        "# Append issue_count to vacols\n",
        "vacols[\"issue_count\"] = bva[\"issue_count\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "colab_type": "code",
        "id": "y7wtXv9wHugy",
        "outputId": "52e1a6f7-6f2b-4a39-82bb-cd45c4dff9ef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "There are 346,915 single issue documents in corpus.\n"
          ]
        }
      ],
      "source": [
        "# Find set of documents which are single issue\n",
        "single_issue_citations = np.array(vacols[vacols.issue_count == 1].tiread2)\n",
        "single_issue_documents = documents[np.isin(documents_int, single_issue_citations)]\n",
        "print(f\"There are {len(single_issue_documents):,} single issue documents in corpus.\")\n",
        "vacols_single_issue = vacols[vacols.issue_count == 1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "colab_type": "code",
        "id": "iOClzoqNHVBd",
        "outputId": "5b56acee-d3cd-4b42-9918-2010a357dac1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "There are 12 tiread2 values with more than 1 row in metadata.\n"
          ]
        }
      ],
      "source": [
        "# There are some rows with duplicate tiread2\n",
        "# Keep the row with the earlier imgadtm date\n",
        "vacols_single_issue = vacols_single_issue.sort_values([\"tiread2\", \"imgadtm\"])\n",
        "dups = vacols_single_issue[vacols_single_issue.tiread2.duplicated(keep=\"first\")]\n",
        "print(f\"There are {dups.shape[0]} tiread2 values with more than 1 row in metadata.\")\n",
        "\n",
        "non_dups = vacols_single_issue[~vacols_single_issue.tiread2.duplicated(keep=False)]\n",
        "vacols_dedup = pd.concat((non_dups, dups), ignore_index=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "colab_type": "code",
        "id": "wydCXcGPMVi3",
        "outputId": "ebd23299-64f8-444c-c299-abd44c003085"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Copying file://vacols_processed.csv [Content-Type=text/csv]...\n",
            "-\n",
            "Operation completed over 1 objects/25.2 MiB.                                     \n"
          ]
        }
      ],
      "source": [
        "# Upload vacols\n",
        "vacols_dedup.to_csv(\"vacols_processed.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "XsMI_OrIMyKI"
      },
      "source": [
        "This section writes all the single-issue BVA decision documents into avro format and uploads to GCP. Avro format is suitable for reading and processing by Dask."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "colab_type": "code",
        "id": "d-1_GTlUsnEC",
        "outputId": "12da8d41-99f7-4b4b-fc4d-cc64375b1e28"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘single-issue-decisions-avro’: File exists\n",
            "Time elapsed: 437.37 seconds.\n"
          ]
        }
      ],
      "source": [
        "# Start Dask Client\n",
        "cluster = LocalCluster(processes=False, n_workers=12, threads_per_worker=1, diagnostics_port=None)\n",
        "client = Client(cluster)\n",
        "client\n",
        "\n",
        "# Avro Schema for Storing Documents\n",
        "schema = {'name': 'all-bva-decisions',\n",
        "          'namespace': 'Documents',\n",
        "          'doc': 'Full case documents for all BVA decisions',\n",
        "          'type': 'record',\n",
        "          'fields': [{'name': 'text', 'type': 'string'},\n",
        "                     {'name': 'bva_id', 'type': 'int'}]}\n",
        "\n",
        "# Write documents to Avro (compressed format)\n",
        "timer = Timer()\n",
        "timer.start()\n",
        "folder = \"all-bva-decisions\"\n",
        "list_files = [f\"{folder}/{x}\" for x in single_issue_documents]\n",
        "loaded_files = load_case_documents(list_files)\n",
        "!mkdir single-issue-decisions-avro\n",
        "loaded_files.to_avro(\"single-issue-decisions-avro/decisions.*.avro\", schema=schema, codec='deflate')\n",
        "timer.end()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyP6sToZFMdzXQfRGaFLmOnk",
      "collapsed_sections": [],
      "include_colab_link": true,
      "name": "clean_single_issue_data.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
